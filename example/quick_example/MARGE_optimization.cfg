# For details on these parameters, see the MARGE_rangetest.cfg or MARGE_run.cfg 
# files.  This file will only detail the Bayesian optimization parameters.

[DEFAULT]
resume = False
seed = 0
verb = 2

[MARGE]
datagenfile = datagen
datagen = False
cfile = None
processdat = False
preservedat = True

NNModel   = True
trainflag = True
validflag = True
testflag  = False

# Number of models to train per GPU during the optimization
optimize = 200

# Number of GPUs to use for the optimization
optngpus = 1

# Minimum and maximum number of layers to consider during the optimization
optnlays = 1 3

# Layer types to use.  Specify as many as the maximum for `optnlays`.
# These are used in the order they are specified.  So, if a model with 2 layers 
# is chosen, then it will use the first 2 layers specified here.
# If you want to do something different, such as changing the first layer but 
# keeping the last N layers the same, then you will need to run separate 
# optimizations.
optlayer = dense dense dense

# The allowed values for layers with nodes.  For each layer with nodes, a  
# value from this list will be chosen.  Values of 2^n are specified here since  
# GPUs are optimized for calculations of that size.
optnnode = 2 4 8 16 32 64 128 256

# The maximum number of nodes (feature maps) for convolutional layers.  In this 
# example, any conv layer would have at most 128 feature maps.  This is enabled 
# because model size can balloon quickly if you have a large number of 
# feature maps, leading to out-of-memory errors.  In general, start with this 
# value equal to your maximum of `optnnode`, then reduce it if you hit OOM errors.
optmaxconvnode = 128

# The activations functions to consider during the optimization.  Each layer has 
# its own activation function selected from this list.
optactiv = relu elu leakyrelu sig tanh

# For activation functions with parameters, this sets the minimum and maximum values 
# that can be chosen for it.
optactrng = 0.01 0.6

# If you wish to also optimize the learning rate at the same time, you can 
# use these parameters to set the minimum and maximum LR.  Note that it can 
# be very, very costly to do that, since it will require training a lot more 
# models.  It is generally recommended to not use this, and instead run multiple 
# optimizations for different learning rate policies (specified near the end 
# of this file, using lengthscale/min_lr and max_lr).
optminlr = None
optmaxlr = None

TFR_file = circle
buffer = 15
ncores = 6

normalize = False
scale     = True
scalelims = -1, 1

inputdir  = inputs_quickexample
outputdir = outputs_optimization
plotdir   = plots
datadir   = ../data
preddir   = pred

ishape = 1
oshape = 2
ilog = False
olog = False

xvals = None
xlabel = None
ylabel = None

fxmean = xmean.npy
fymean = ymean.npy
fxstd = xstd.npy
fystd = ystd.npy
fxmin = xmin.npy
fxmax = xmax.npy
fymin = ymin.npy
fymax = ymax.npy
fsize = datsize.npy
rmse_file = rmse
r2_file = r2
statsaxes = all

weight_file = nn_weights.keras

gridsearch = False

nodes = 64

activations = elu

act_params =  None

layers = dense

lay_params = None

epochs     = 60
patience   = 60
batch_size = 256

lengthscale = 1e-3  # Can also call this parameter min_lr
max_lr      = 1e-1

clr_mode  = triangular2
clr_steps = 6

plot_cases = None
smoothing  = 0
